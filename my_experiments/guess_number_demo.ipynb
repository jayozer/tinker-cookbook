{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RLVR: Teaching an LLM to Play \"Guess the Number\"\n",
    "\n",
    "A fun multi-turn RL demo where we teach a model to play the classic guessing game!\n",
    "\n",
    "**The Game:**\n",
    "- Secret number between 0 and 1024\n",
    "- Model gets 10 guesses\n",
    "- After each guess: \"Too high\", \"Too low\", or \"Correct!\"\n",
    "- Optimal strategy: Binary search (can always win in 10 guesses since 2^10 = 1024)\n",
    "\n",
    "**Why this is interesting:**\n",
    "- Models often DON'T do perfect binary search\n",
    "- Multi-turn reasoning is hard\n",
    "- We can watch the model learn to be more strategic!\n",
    "\n",
    "---\n",
    "\n",
    "## What is Tinker?\n",
    "\n",
    "**Tinker** is a training API that separates concerns:\n",
    "\n",
    "| You Handle (CPU) | Tinker Handles (GPUs) |\n",
    "|------------------|----------------------|\n",
    "| Training loop logic | Distributed training |\n",
    "| Data/environment | GPU orchestration |\n",
    "| Reward functions | Forward/backward passes |\n",
    "| When to update | Gradient accumulation |\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                        YOUR MACHINE (CPU)                       â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
    "â”‚  â”‚   Game      â”‚â”€â”€â”€â†’â”‚  Reward     â”‚â”€â”€â”€â†’â”‚  Training Loop      â”‚  â”‚\n",
    "â”‚  â”‚ Environment â”‚    â”‚  Function   â”‚    â”‚  (this notebook!)   â”‚  â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
    "â”‚                                                   â”‚ API calls   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                                    â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                      TINKER CLOUD (GPUs)                        â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
    "â”‚  â”‚  Sampling   â”‚    â”‚  Forward/   â”‚    â”‚  Optimizer          â”‚  â”‚\n",
    "â”‚  â”‚  (generate) â”‚    â”‚  Backward   â”‚    â”‚  (update weights)   â”‚  â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "## What is RLVR?\n",
    "\n",
    "**Reinforcement Learning with Verifiable Rewards** - RL where we can programmatically check if the answer is correct:\n",
    "\n",
    "| RLHF (Human Feedback) | RLVR (Verifiable Rewards) |\n",
    "|----------------------|---------------------------|\n",
    "| Needs human labelers | Automated reward function |\n",
    "| Expensive & slow | Fast & scalable |\n",
    "| Subjective | Objective ground truth |\n",
    "| \"Is this helpful?\" | \"Did you win the game?\" |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import tinker\n",
    "from tinker import types\n",
    "from tinker.types.tensor_data import TensorData\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Load API key from .env\n",
    "load_dotenv()\n",
    "assert os.getenv(\"TINKER_API_KEY\"), \"Please set TINKER_API_KEY in your .env file\"\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ”‘ Key Imports Explained\n",
    "\n",
    "```python\n",
    "import tinker                          # Main Tinker SDK\n",
    "from tinker import types               # Type definitions (Datum, SamplingParams, etc.)\n",
    "from tinker.types.tensor_data import TensorData  # For passing tensors to API\n",
    "```\n",
    "\n",
    "**Tinker's Core Types:**\n",
    "- `ServiceClient` - Entry point, creates training/sampling clients\n",
    "- `TrainingClient` - Manages model weights, runs forward/backward passes\n",
    "- `SamplingClient` - Generates text from current model weights\n",
    "- `Datum` - A single training example (input + loss function inputs)\n",
    "- `ModelInput` - Tokenized prompt for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Qwen/Qwen3-4B-Instruct-2507\n",
      "Game: Guess a number 0-1023 in 10 tries\n",
      "Optimal strategy: Binary search (always wins since 2^10 = 1024)\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "MODEL_NAME = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "\n",
    "LORA_RANK = 32\n",
    "LEARNING_RATE = 4e-5\n",
    "BATCH_SIZE = 4           # Games per batch\n",
    "GROUP_SIZE = 4           # Parallel attempts per game\n",
    "MAX_TOKENS = 32          # Max tokens per guess\n",
    "N_TRAINING_BATCHES = 5\n",
    "MAX_GUESSES = 10         # Guesses allowed per game\n",
    "NUMBER_RANGE = 1024      # Guess 0 to NUMBER_RANGE-1\n",
    "\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Game: Guess a number 0-{NUMBER_RANGE-1} in {MAX_GUESSES} tries\")\n",
    "print(f\"Optimal strategy: Binary search (always wins since 2^{MAX_GUESSES} = {2**MAX_GUESSES})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“Š Hyperparameters Explained\n",
    "\n",
    "| Parameter | Value | Why? |\n",
    "|-----------|-------|------|\n",
    "| `LORA_RANK=32` | LoRA adapter rank | Lower = fewer params, faster. Higher = more expressive |\n",
    "| `LEARNING_RATE=4e-5` | ~10x higher than full fine-tuning | LoRA needs higher LR since fewer params are updated |\n",
    "| `GROUP_SIZE=4` | Games per secret number | For GRPO-style variance reduction (compare wins vs losses on same problem) |\n",
    "| `BATCH_SIZE=4` | Different secret numbers per batch | More diversity in training signal |\n",
    "\n",
    "**What is LoRA?**\n",
    "\n",
    "Instead of updating all model weights, LoRA adds small \"adapter\" matrices:\n",
    "\n",
    "```\n",
    "Original:     W (frozen, 4B params)\n",
    "LoRA:         W + AÂ·B (A and B are small, ~1M params)\n",
    "                   â†‘\n",
    "            rank=32 controls size\n",
    "```\n",
    "\n",
    "This makes training 10-100x more efficient!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Tinker!\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer and Tinker client\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "service_client = tinker.ServiceClient()\n",
    "\n",
    "EOS_TOKEN_ID = tokenizer.eos_token_id\n",
    "print(f\"Connected to Tinker!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ”— Tinker Client Hierarchy\n",
    "\n",
    "```\n",
    "ServiceClient (entry point)\n",
    "    â”‚\n",
    "    â”œâ”€â”€â–¶ create_lora_training_client()\n",
    "    â”‚         â”‚\n",
    "    â”‚         â–¼\n",
    "    â”‚    TrainingClient\n",
    "    â”‚         â”‚\n",
    "    â”‚         â”œâ”€â”€â–¶ forward_backward()     # Compute gradients\n",
    "    â”‚         â”œâ”€â”€â–¶ optim_step()           # Update weights  \n",
    "    â”‚         â””â”€â”€â–¶ save_weights_and_get_sampling_client()\n",
    "    â”‚                   â”‚\n",
    "    â”‚                   â–¼\n",
    "    â”‚              SamplingClient\n",
    "    â”‚                   â”‚\n",
    "    â”‚                   â””â”€â”€â–¶ sample()     # Generate text\n",
    "    â”‚\n",
    "    â””â”€â”€â–¶ create_sampling_client()         # For inference-only\n",
    "```\n",
    "\n",
    "**The key insight:** Training and sampling are separate concerns. You get a new `SamplingClient` after each weight update to sample from the updated model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: The Guessing Game Environment\n",
    "\n",
    "### Multi-Turn RL vs Single-Turn\n",
    "\n",
    "Most RLVR examples are **single-turn**: one prompt â†’ one response â†’ one reward.\n",
    "\n",
    "This game is **multi-turn**: the model takes multiple actions before getting a reward!\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                     SINGLE-TURN RLVR                             â”‚\n",
    "â”‚                                                                  â”‚\n",
    "â”‚   Prompt â”€â”€â†’ Model â”€â”€â†’ Response â”€â”€â†’ Reward                       â”‚\n",
    "â”‚   \"2+2=?\"      â”‚       \"4\"           âœ“ 1.0                       â”‚\n",
    "â”‚                â”‚                                                 â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                 â”‚\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                â–¼    MULTI-TURN RLVR (This Demo!)                â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚   Turn 1: \"Guess?\" â”€â”€â†’ \"512\"  â”€â”€â†’ \"Too low\"                     â”‚\n",
    "â”‚   Turn 2: \"Guess?\" â”€â”€â†’ \"768\"  â”€â”€â†’ \"Too high\"                    â”‚\n",
    "â”‚   Turn 3: \"Guess?\" â”€â”€â†’ \"640\"  â”€â”€â†’ \"Correct!\" â”€â”€â†’ Reward: 1.0    â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â”‚   All 3 actions get credit for the win!                         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### The Challenge: Credit Assignment\n",
    "\n",
    "When we win, which guess deserves credit?\n",
    "- The final correct guess? \n",
    "- The earlier guesses that narrowed the range?\n",
    "- **Answer: All of them!** We assign the same reward to every action in a winning trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game environment defined!\n",
      "\n",
      "Example system prompt:\n",
      "You are playing a number guessing game. \n",
      "- I'm thinking of a number between 0 and 1023\n",
      "- You have 10 guesses to find it\n",
      "- After each guess, I'll tell you if it's too high, too low, or correct\n",
      "- Output...\n"
     ]
    }
   ],
   "source": [
    "SYSTEM_PROMPT = f\"\"\"You are playing a number guessing game. \n",
    "- I'm thinking of a number between 0 and {NUMBER_RANGE-1}\n",
    "- You have {MAX_GUESSES} guesses to find it\n",
    "- After each guess, I'll tell you if it's too high, too low, or correct\n",
    "- Output your guess in the format: Guess: <number>\n",
    "- Just output the guess, nothing else.\"\"\"\n",
    "\n",
    "@dataclass\n",
    "class GameState:\n",
    "    \"\"\"Tracks the state of a guessing game.\"\"\"\n",
    "    secret: int\n",
    "    guesses: list  # List of (guess, feedback) tuples\n",
    "    won: bool = False\n",
    "    \n",
    "    def is_done(self) -> bool:\n",
    "        return self.won or len(self.guesses) >= MAX_GUESSES\n",
    "    \n",
    "    def get_reward(self) -> float:\n",
    "        return 1.0 if self.won else 0.0\n",
    "\n",
    "def parse_guess(response: str) -> int | None:\n",
    "    \"\"\"Extract the guessed number from model response.\"\"\"\n",
    "    # Look for \"Guess: <number>\" pattern\n",
    "    match = re.search(r'Guess:\\s*(\\d+)', response)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    # Fallback: find any number\n",
    "    numbers = re.findall(r'\\d+', response)\n",
    "    if numbers:\n",
    "        return int(numbers[0])\n",
    "    return None\n",
    "\n",
    "def get_feedback(guess: int, secret: int) -> tuple[str, bool]:\n",
    "    \"\"\"Return feedback and whether the guess was correct.\"\"\"\n",
    "    if guess == secret:\n",
    "        return \"Correct! You win!\", True\n",
    "    elif guess < secret:\n",
    "        return \"Too low.\", False\n",
    "    else:\n",
    "        return \"Too high.\", False\n",
    "\n",
    "def build_conversation(game: GameState) -> list[dict]:\n",
    "    \"\"\"Build the conversation history for the model.\"\"\"\n",
    "    messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n",
    "    \n",
    "    for guess, feedback in game.guesses:\n",
    "        messages.append({\"role\": \"assistant\", \"content\": f\"Guess: {guess}\"})\n",
    "        messages.append({\"role\": \"user\", \"content\": feedback})\n",
    "    \n",
    "    return messages\n",
    "\n",
    "def build_model_input(game: GameState) -> types.ModelInput:\n",
    "    \"\"\"Build tokenized model input from game state.\"\"\"\n",
    "    messages = build_conversation(game)\n",
    "    prompt_text = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    tokens = tokenizer.encode(prompt_text, add_special_tokens=False)\n",
    "    return types.ModelInput.from_ints(tokens=tokens)\n",
    "\n",
    "print(\"Game environment defined!\")\n",
    "print(f\"\\nExample system prompt:\\n{SYSTEM_PROMPT[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Play a Demo Game (Before Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created training client with LoRA rank 32\n",
      "Ready to play!\n"
     ]
    }
   ],
   "source": [
    "# Create training client\n",
    "training_client = service_client.create_lora_training_client(\n",
    "    base_model=MODEL_NAME,\n",
    "    rank=LORA_RANK\n",
    ")\n",
    "print(f\"Created training client with LoRA rank {LORA_RANK}\")\n",
    "\n",
    "# Get initial sampling client\n",
    "initial_sampler = training_client.save_weights_and_get_sampling_client(name=\"initial\")\n",
    "print(\"Ready to play!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ® Key API: `sample()`\n",
    "\n",
    "The sampling client generates text from the model:\n",
    "\n",
    "```python\n",
    "result = sampling_client.sample(\n",
    "    prompt=model_input,           # Tokenized input (ModelInput)\n",
    "    num_samples=1,                # How many completions to generate\n",
    "    sampling_params=SamplingParams(\n",
    "        max_tokens=32,            # Max tokens to generate\n",
    "        temperature=0.0,          # 0=greedy, >0=random\n",
    "        stop=[EOS_TOKEN_ID]       # Stop tokens\n",
    "    )\n",
    ").result()                        # .result() waits for completion\n",
    "\n",
    "# Access the generated tokens and their log probabilities\n",
    "tokens = result.sequences[0].tokens      # List[int]\n",
    "logprobs = result.sequences[0].logprobs  # List[float] - needed for training!\n",
    "```\n",
    "\n",
    "**Why do we need logprobs?**\n",
    "\n",
    "For policy gradient training, we need to know how likely the model thought each token was. This is used in the importance sampling loss to correct for distribution shift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "DEMO GAME (Before Training)\n",
      "==================================================\n",
      "\n",
      "ğŸ¯ Secret number: 640\n",
      "----------------------------------------\n",
      "Turn 1: Guess 512 â†’ Too low.\n",
      "Turn 2: Guess 768 â†’ Too high.\n",
      "Turn 3: Guess 640 âœ“ Correct! You win!\n",
      "----------------------------------------\n",
      "Result: ğŸ‰ WON in 3 turns\n"
     ]
    }
   ],
   "source": [
    "def play_game(sampling_client, secret: int, verbose: bool = True) -> GameState:\n",
    "    \"\"\"Play a complete guessing game and return the final state.\"\"\"\n",
    "    game = GameState(secret=secret, guesses=[])\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nğŸ¯ Secret number: {secret}\")\n",
    "        print(\"-\" * 40)\n",
    "    \n",
    "    while not game.is_done():\n",
    "        # Get model's guess\n",
    "        model_input = build_model_input(game)\n",
    "        result = sampling_client.sample(\n",
    "            prompt=model_input,\n",
    "            num_samples=1,\n",
    "            sampling_params=types.SamplingParams(\n",
    "                max_tokens=MAX_TOKENS,\n",
    "                temperature=0.0,\n",
    "                stop=[EOS_TOKEN_ID] if EOS_TOKEN_ID else []\n",
    "            )\n",
    "        ).result()\n",
    "        \n",
    "        response = tokenizer.decode(result.sequences[0].tokens, skip_special_tokens=True)\n",
    "        guess = parse_guess(response)\n",
    "        \n",
    "        if guess is None:\n",
    "            if verbose:\n",
    "                print(f\"Turn {len(game.guesses)+1}: Failed to parse guess from '{response[:50]}'\")\n",
    "            game.guesses.append((0, \"Invalid guess. Try again.\"))\n",
    "            continue\n",
    "        \n",
    "        feedback, correct = get_feedback(guess, secret)\n",
    "        game.guesses.append((guess, feedback))\n",
    "        game.won = correct\n",
    "        \n",
    "        if verbose:\n",
    "            status = \"âœ“\" if correct else \"â†’\"\n",
    "            print(f\"Turn {len(game.guesses)}: Guess {guess} {status} {feedback}\")\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"Result: {'ğŸ‰ WON' if game.won else 'âŒ LOST'} in {len(game.guesses)} turns\")\n",
    "    \n",
    "    return game\n",
    "\n",
    "# Play a demo game\n",
    "print(\"=\" * 50)\n",
    "print(\"DEMO GAME (Before Training)\")\n",
    "print(\"=\" * 50)\n",
    "demo_game = play_game(initial_sampler, secret=640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating BEFORE training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e79be92929b744ce9e9e23b1fb5e8dde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results over 20 games:\n",
      "  Win rate: 2/20 = 10.0%\n",
      "  Avg guesses: 10.0\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(sampling_client, n_games: int = 20) -> float:\n",
    "    \"\"\"Evaluate win rate over multiple games.\"\"\"\n",
    "    wins = 0\n",
    "    total_guesses = 0\n",
    "    \n",
    "    random.seed(42)\n",
    "    secrets = [random.randint(0, NUMBER_RANGE-1) for _ in range(n_games)]\n",
    "    \n",
    "    for secret in tqdm(secrets, desc=\"Evaluating\"):\n",
    "        game = play_game(sampling_client, secret, verbose=False)\n",
    "        wins += 1 if game.won else 0\n",
    "        total_guesses += len(game.guesses)\n",
    "    \n",
    "    win_rate = wins / n_games\n",
    "    avg_guesses = total_guesses / n_games\n",
    "    \n",
    "    print(f\"\\nResults over {n_games} games:\")\n",
    "    print(f\"  Win rate: {wins}/{n_games} = {win_rate:.1%}\")\n",
    "    print(f\"  Avg guesses: {avg_guesses:.1f}\")\n",
    "    \n",
    "    return win_rate\n",
    "\n",
    "print(\"Evaluating BEFORE training...\")\n",
    "before_win_rate = evaluate_model(initial_sampler, n_games=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: RLVR Training\n",
    "\n",
    "### The Training Loop\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    RLVR TRAINING LOOP                               â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                     â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
    "â”‚  â”‚ 1. GET SAMPLING CLIENT                                       â”‚   â”‚\n",
    "â”‚  â”‚    sampling_client = training_client                         â”‚   â”‚\n",
    "â”‚  â”‚        .save_weights_and_get_sampling_client()               â”‚   â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
    "â”‚                              â”‚                                      â”‚\n",
    "â”‚                              â–¼                                      â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
    "â”‚  â”‚ 2. COLLECT TRAJECTORIES                                      â”‚   â”‚\n",
    "â”‚  â”‚    For each game:                                            â”‚   â”‚\n",
    "â”‚  â”‚      - Play until win/lose                                   â”‚   â”‚\n",
    "â”‚  â”‚      - Record (prompt, response, logprobs) for each turn     â”‚   â”‚\n",
    "â”‚  â”‚      - Get final reward (1.0 = win, 0.0 = lose)              â”‚   â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
    "â”‚                              â”‚                                      â”‚\n",
    "â”‚                              â–¼                                      â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
    "â”‚  â”‚ 3. COMPUTE ADVANTAGES (GRPO-style)                           â”‚   â”‚\n",
    "â”‚  â”‚    advantage = reward - mean(rewards)                        â”‚   â”‚\n",
    "â”‚  â”‚                                                              â”‚   â”‚\n",
    "â”‚  â”‚    Win:  advantage = 1.0 - 0.5 = +0.5  (reinforce!)          â”‚   â”‚\n",
    "â”‚  â”‚    Lose: advantage = 0.0 - 0.5 = -0.5  (discourage!)         â”‚   â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
    "â”‚                              â”‚                                      â”‚\n",
    "â”‚                              â–¼                                      â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
    "â”‚  â”‚ 4. BUILD TRAINING DATA                                       â”‚   â”‚\n",
    "â”‚  â”‚    datum = Datum(                                            â”‚   â”‚\n",
    "â”‚  â”‚        model_input = prompt + response,                      â”‚   â”‚\n",
    "â”‚  â”‚        loss_fn_inputs = {                                    â”‚   â”‚\n",
    "â”‚  â”‚            \"target_tokens\": response tokens,                 â”‚   â”‚\n",
    "â”‚  â”‚            \"logprobs\": sampling logprobs,                    â”‚   â”‚\n",
    "â”‚  â”‚            \"advantages\": reward signal                       â”‚   â”‚\n",
    "â”‚  â”‚        }                                                     â”‚   â”‚\n",
    "â”‚  â”‚    )                                                         â”‚   â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
    "â”‚                              â”‚                                      â”‚\n",
    "â”‚                              â–¼                                      â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚\n",
    "â”‚  â”‚ 5. UPDATE MODEL                                              â”‚   â”‚\n",
    "â”‚  â”‚    training_client.forward_backward(datums, loss_fn=...)     â”‚   â”‚\n",
    "â”‚  â”‚    training_client.optim_step(adam_params)                   â”‚   â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚\n",
    "â”‚                              â”‚                                      â”‚\n",
    "â”‚                              â–¼                                      â”‚\n",
    "â”‚                         REPEAT                                      â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Key API: `forward_backward()`\n",
    "\n",
    "This is where the magic happens! Tinker computes gradients on GPUs:\n",
    "\n",
    "```python\n",
    "training_client.forward_backward(\n",
    "    datums,                          # List of training examples\n",
    "    loss_fn=\"importance_sampling\"    # Policy gradient loss\n",
    ")\n",
    "```\n",
    "\n",
    "**Loss function options:**\n",
    "| Loss | Use Case |\n",
    "|------|----------|\n",
    "| `\"cross_entropy\"` | Supervised learning (SFT) |\n",
    "| `\"importance_sampling\"` | Basic policy gradient (REINFORCE) |\n",
    "| `\"ppo\"` | PPO with clipping |\n",
    "\n",
    "### Key API: `optim_step()`\n",
    "\n",
    "Updates the model weights using accumulated gradients:\n",
    "\n",
    "```python\n",
    "training_client.optim_step(\n",
    "    AdamParams(learning_rate=4e-5, beta1=0.9, beta2=0.95)\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training params configured!\n"
     ]
    }
   ],
   "source": [
    "# Training hyperparameters\n",
    "adam_params = types.AdamParams(\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    beta1=0.9,\n",
    "    beta2=0.95,\n",
    "    eps=1e-8\n",
    ")\n",
    "\n",
    "sampling_params = types.SamplingParams(\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    temperature=0.7,  # Exploration during training\n",
    "    stop=[EOS_TOKEN_ID] if EOS_TOKEN_ID else []\n",
    ")\n",
    "\n",
    "training_metrics = []\n",
    "print(\"Training params configured!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trajectory collector defined!\n"
     ]
    }
   ],
   "source": [
    "def collect_game_trajectory(sampling_client, secret: int) -> tuple[list, float]:\n",
    "    \"\"\"\n",
    "    Play a game and collect all (prompt, response, logprobs) for training.\n",
    "    Returns (trajectory, reward)\n",
    "    \"\"\"\n",
    "    game = GameState(secret=secret, guesses=[])\n",
    "    trajectory = []  # List of (model_input, tokens, logprobs)\n",
    "    \n",
    "    while not game.is_done():\n",
    "        model_input = build_model_input(game)\n",
    "        \n",
    "        result = sampling_client.sample(\n",
    "            prompt=model_input,\n",
    "            num_samples=1,\n",
    "            sampling_params=sampling_params\n",
    "        ).result()\n",
    "        \n",
    "        seq = result.sequences[0]\n",
    "        response = tokenizer.decode(seq.tokens, skip_special_tokens=True)\n",
    "        guess = parse_guess(response)\n",
    "        \n",
    "        # Store trajectory data\n",
    "        trajectory.append((model_input, seq.tokens, seq.logprobs))\n",
    "        \n",
    "        if guess is None:\n",
    "            game.guesses.append((0, \"Invalid guess.\"))\n",
    "            continue\n",
    "        \n",
    "        feedback, correct = get_feedback(guess, secret)\n",
    "        game.guesses.append((guess, feedback))\n",
    "        game.won = correct\n",
    "    \n",
    "    return trajectory, game.get_reward()\n",
    "\n",
    "print(\"Trajectory collector defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting RLVR training...\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6271e9c9b5064e1ea7a106eb0d0886d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 1/5:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: Win Rate = 31.2% | Training samples = 154\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a14d8dfc9e947bda21e0087b9390110",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 2/5:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 2: Win Rate = 50.0% | Training samples = 140\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d49282b68a2949a8af255c56366dc233",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 3/5:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 3: Win Rate = 0.0% | Training samples = 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0b2f168d3394542b29fcebd5b2117d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 4/5:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4: Win Rate = 56.2% | Training samples = 141\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30397fca6a6d4a8585cc5754953317b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 5/5:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 5: Win Rate = 25.0% | Training samples = 160\n",
      "==================================================\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting RLVR training...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for batch_idx in range(N_TRAINING_BATCHES):\n",
    "    # Get sampling client for current weights\n",
    "    sampling_client = training_client.save_weights_and_get_sampling_client(\n",
    "        name=f\"step_{batch_idx}\"\n",
    "    )\n",
    "    \n",
    "    # Generate random secrets for this batch\n",
    "    secrets = [random.randint(0, NUMBER_RANGE-1) for _ in range(BATCH_SIZE)]\n",
    "    \n",
    "    # Collect trajectories for multiple games\n",
    "    all_trajectories = []  # (trajectory, reward) pairs\n",
    "    \n",
    "    for secret in tqdm(secrets, desc=f\"Batch {batch_idx+1}/{N_TRAINING_BATCHES}\"):\n",
    "        # Play GROUP_SIZE games with same secret for variance reduction\n",
    "        for _ in range(GROUP_SIZE):\n",
    "            traj, reward = collect_game_trajectory(sampling_client, secret)\n",
    "            all_trajectories.append((traj, reward))\n",
    "    \n",
    "    # Compute rewards and advantages\n",
    "    rewards = [r for _, r in all_trajectories]\n",
    "    mean_reward = sum(rewards) / len(rewards)\n",
    "    \n",
    "    # Build training datums from trajectories\n",
    "    datums = []\n",
    "    \n",
    "    for trajectory, reward in all_trajectories:\n",
    "        advantage = reward - mean_reward\n",
    "        \n",
    "        # Skip if no learning signal\n",
    "        if advantage == 0:\n",
    "            continue\n",
    "        \n",
    "        # Create datum for each turn in the trajectory\n",
    "        for model_input, tokens, logprobs in trajectory:\n",
    "            if not tokens or logprobs is None:\n",
    "                continue\n",
    "            \n",
    "            ob_len = model_input.length - 1\n",
    "            \n",
    "            # Build full sequence\n",
    "            full_input = model_input.append(\n",
    "                types.EncodedTextChunk(tokens=tokens[:-1] if len(tokens) > 1 else tokens)\n",
    "            )\n",
    "            \n",
    "            # Pad tensors\n",
    "            target_tokens = [0] * ob_len + list(tokens)\n",
    "            padded_logprobs = [0.0] * ob_len + list(logprobs)\n",
    "            padded_advantages = [0.0] * ob_len + [advantage] * len(tokens)\n",
    "            \n",
    "            # Truncate to match\n",
    "            seq_len = full_input.length\n",
    "            \n",
    "            datum = types.Datum(\n",
    "                model_input=full_input,\n",
    "                loss_fn_inputs={\n",
    "                    \"target_tokens\": TensorData.from_torch(torch.tensor(target_tokens[:seq_len])),\n",
    "                    \"logprobs\": TensorData.from_torch(torch.tensor(padded_logprobs[:seq_len])),\n",
    "                    \"advantages\": TensorData.from_torch(torch.tensor(padded_advantages[:seq_len])),\n",
    "                }\n",
    "            )\n",
    "            datums.append(datum)\n",
    "    \n",
    "    # Training step\n",
    "    if datums:\n",
    "        fwd_bwd_future = training_client.forward_backward(datums, loss_fn=\"importance_sampling\")\n",
    "        optim_future = training_client.optim_step(adam_params)\n",
    "        fwd_bwd_future.result()\n",
    "        optim_future.result()\n",
    "    \n",
    "    # Log metrics\n",
    "    training_metrics.append({\n",
    "        \"batch\": batch_idx,\n",
    "        \"win_rate\": mean_reward,\n",
    "        \"n_datums\": len(datums)\n",
    "    })\n",
    "    \n",
    "    print(f\"Batch {batch_idx+1}: Win Rate = {mean_reward:.1%} | Training samples = {len(datums)}\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Evaluate After Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating AFTER training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6bf7c8c104749e38bd6f374ca2e5bdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results over 20 games:\n",
      "  Win rate: 3/20 = 15.0%\n",
      "  Avg guesses: 10.0\n"
     ]
    }
   ],
   "source": [
    "# Get final sampling client\n",
    "final_sampler = training_client.save_weights_and_get_sampling_client(name=\"final\")\n",
    "\n",
    "print(\"Evaluating AFTER training...\")\n",
    "after_win_rate = evaluate_model(final_sampler, n_games=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "DEMO GAME (After Training)\n",
      "==================================================\n",
      "\n",
      "ğŸ¯ Secret number: 640\n",
      "----------------------------------------\n",
      "Turn 1: Guess 512 â†’ Too low.\n",
      "Turn 2: Guess 768 â†’ Too high.\n",
      "Turn 3: Guess 640 âœ“ Correct! You win!\n",
      "----------------------------------------\n",
      "Result: ğŸ‰ WON in 3 turns\n"
     ]
    }
   ],
   "source": [
    "# Play a demo game with trained model\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"DEMO GAME (After Training)\")\n",
    "print(\"=\" * 50)\n",
    "demo_game_after = play_game(final_sampler, secret=640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "RLVR Training Summary: Guess the Number\n",
      "==================================================\n",
      "Model: Qwen/Qwen3-4B-Instruct-2507\n",
      "Game: Guess 0-1023 in 10 tries\n",
      "Training: 5 batches\n",
      "\n",
      "Win Rate:\n",
      "  Before: 10.0%\n",
      "  After:  15.0%\n",
      "  Change: +5.0% ğŸ‰\n",
      "\n",
      "Training Progress:\n",
      "  Batch 1: 31.2% â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "  Batch 2: 50.0% â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "  Batch 3: 0.0% \n",
      "  Batch 4: 56.2% â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "  Batch 5: 25.0% â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n"
     ]
    }
   ],
   "source": [
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"RLVR Training Summary: Guess the Number\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Game: Guess 0-{NUMBER_RANGE-1} in {MAX_GUESSES} tries\")\n",
    "print(f\"Training: {N_TRAINING_BATCHES} batches\")\n",
    "print(f\"\\nWin Rate:\")\n",
    "print(f\"  Before: {before_win_rate:.1%}\")\n",
    "print(f\"  After:  {after_win_rate:.1%}\")\n",
    "improvement = after_win_rate - before_win_rate\n",
    "print(f\"  Change: {improvement:+.1%} {'ğŸ‰' if improvement > 0 else ''}\")\n",
    "print(\"\\nTraining Progress:\")\n",
    "for m in training_metrics:\n",
    "    bar = \"â–ˆ\" * int(m['win_rate'] * 20)\n",
    "    print(f\"  Batch {m['batch']+1}: {m['win_rate']:.1%} {bar}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Tinker API Recap\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                     TINKER API CHEATSHEET                      â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                â”‚\n",
    "â”‚  SETUP                                                         â”‚\n",
    "â”‚  â”€â”€â”€â”€â”€                                                         â”‚\n",
    "â”‚  service_client = tinker.ServiceClient()                       â”‚\n",
    "â”‚  training_client = service_client.create_lora_training_client( â”‚\n",
    "â”‚      base_model=\"Qwen/Qwen3-4B-Instruct-2507\",                 â”‚\n",
    "â”‚      rank=32                                                   â”‚\n",
    "â”‚  )                                                             â”‚\n",
    "â”‚                                                                â”‚\n",
    "â”‚  SAMPLING                                                      â”‚\n",
    "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€                                                      â”‚\n",
    "â”‚  sampler = training_client.save_weights_and_get_sampling_clientâ”‚\n",
    "â”‚  result = sampler.sample(prompt, num_samples=8, ...)           â”‚\n",
    "â”‚  tokens = result.sequences[0].tokens                           â”‚\n",
    "â”‚  logprobs = result.sequences[0].logprobs                       â”‚\n",
    "â”‚                                                                â”‚\n",
    "â”‚  TRAINING                                                      â”‚\n",
    "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€                                                      â”‚\n",
    "â”‚  datum = Datum(model_input, loss_fn_inputs={                   â”‚\n",
    "â”‚      \"target_tokens\": ...,                                     â”‚\n",
    "â”‚      \"logprobs\": ...,                                          â”‚\n",
    "â”‚      \"advantages\": ...                                         â”‚\n",
    "â”‚  })                                                            â”‚\n",
    "â”‚  training_client.forward_backward(datums, loss_fn=\"ppo\")       â”‚\n",
    "â”‚  training_client.optim_step(AdamParams(lr=4e-5))               â”‚\n",
    "â”‚                                                                â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "| Concept | What It Means |\n",
    "|---------|--------------|\n",
    "| **RLVR** | RL with programmatic rewards (no humans needed!) |\n",
    "| **Multi-turn RL** | Collect full trajectories, assign sparse rewards |\n",
    "| **GRPO** | Compare wins vs losses on same problem for low-variance gradients |\n",
    "| **LoRA** | Efficient fine-tuning by only updating small adapter matrices |\n",
    "| **Importance Sampling** | Correct for policy drift during training |\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "1. **Verifiable rewards scale** - No human labelers needed\n",
    "2. **Multi-turn reasoning** - Teach models to plan ahead  \n",
    "3. **Game playing** - Natural testbed for strategic behavior\n",
    "4. **Binary search** - Optimal strategy the model can discover through RL\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Try the [Tinker Cookbook](https://github.com/thinking-machines-lab/tinker-cookbook) for more recipes\n",
    "- Explore GSM8K math reasoning\n",
    "- Try code generation with unit test rewards\n",
    "- Build your own RL environment!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
